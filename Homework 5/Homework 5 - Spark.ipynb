{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1d7d72",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d65bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 15:31:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff19fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53f102",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5ea3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalileldaou/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/khalileldaou/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c03fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc2bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67f3595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86ba22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7241439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a211c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9001343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 35), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 13, 22), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 11, 43), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 37, 20), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 56, 29), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 57, 47), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23, 9), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 28, 27), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 0, 48), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 7, 12), PUlocationID=129, DOlocationID=129, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 47, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 53, 25), PUlocationID=57, DOlocationID=57, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 10, 6), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 19, 50), PUlocationID=173, DOlocationID=173, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 51, 37), dropOff_datetime=datetime.datetime(2019, 10, 1, 1, 6, 14), PUlocationID=226, DOlocationID=226, SR_Flag=None, Affiliated_base_number='B00021         '),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 28, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 34, 33), PUlocationID=56, DOlocationID=56, SR_Flag=None, Affiliated_base_number='B00021         ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860a93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe97a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv_trips/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d78191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e722a7",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56528a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b300f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "        .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "        .withColumn('dropoff_date', F.to_date(df.dropOff_datetime)) \\\n",
    "        .select('dispatching_base_num', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca63a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.pickup_date == '2019-10-15').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7205555",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8c311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac95940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57e8bf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('fhv_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a38372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|timestampdiff(hour, pickup_datetime, dropOff_datetime)|\n",
      "+------------------------------------------------------+\n",
      "|                                                631152|\n",
      "|                                                631152|\n",
      "|                                                 87672|\n",
      "|                                                 70128|\n",
      "|                                                  8794|\n",
      "|                                                  8784|\n",
      "|                                                  1464|\n",
      "|                                                  1056|\n",
      "|                                                  1056|\n",
      "|                                                   793|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATEDIFF(hour, pickup_datetime, dropOff_datetime)\n",
    "FROM\n",
    "    fhv_trips\n",
    "ORDER BY 1 DESC\n",
    "LIMIT\n",
    "    10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a2a80",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ace1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = pd.read_csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ea0c347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocationID       int64\n",
       "Borough         object\n",
       "Zone            object\n",
       "service_zone    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49e7e9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('LocationID', LongType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_zones).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73a0e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_schema = types.StructType([\n",
    "types.StructField('LocationID', types.IntegerType(), True), \n",
    "types.StructField('Borough', types.StringType(), True), \n",
    "types.StructField('Zone', types.StringType(), True), \n",
    "types.StructField('service_zone', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1db6913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zones_schema) \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3362bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalileldaou/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d501e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|timestampdiff(hour, pickup_datetime, dropOff_datetime)|\n",
      "+------------------------------------------------------+\n",
      "|                                                631152|\n",
      "|                                                631152|\n",
      "|                                                 87672|\n",
      "|                                                 70128|\n",
      "|                                                  8794|\n",
      "|                                                  8784|\n",
      "|                                                  1464|\n",
      "|                                                  1056|\n",
      "|                                                  1056|\n",
      "|                                                   793|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATEDIFF(hour, pickup_datetime, dropOff_datetime)\n",
    "FROM\n",
    "    fhv_trips \n",
    "ORDER BY 1 DESC\n",
    "LIMIT\n",
    "    10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a139adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df.join(df_zones, df.PUlocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "477d649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_dataframe = df_result.groupBy('Zone').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bdb98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5009320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_by_dataframe.sort(asc(\"count\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
